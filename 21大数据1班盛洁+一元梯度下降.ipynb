{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "461620ec-17ba-447c-9be3-8c71f102a09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0:  tensor(0.0121, requires_grad=True) y0:  tensor(0.0001, grad_fn=<PowBackward0>) x0.grad:  tensor(0.0242)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x0 = torch.tensor(5.0, requires_grad=True)\n",
    "y0 = x0**2\n",
    "\n",
    "alpha = 0.1\n",
    "epsilon = 0.0001\n",
    "\n",
    "x_values = []\n",
    "y_values = []\n",
    "\n",
    "# 循环直到y的变化小于epsilon\n",
    "while True:\n",
    "    # 计算梯度\n",
    "    y0.backward()\n",
    "    y = y0.item()  # 保存y0在当前迭代的取值，用于判断迭代的停止条件\n",
    "    # 更新x0\n",
    "    with torch.no_grad():\n",
    "        x0 -= alpha * x0.grad  # x0 = x0-alpha * x0.grad 创建一个新的张量并赋值给x\n",
    "    x0.grad.zero_()\n",
    "    # 计算新的y0\n",
    "    y0 = x0**2\n",
    "    # 检查是否满足结束条件\n",
    "    if abs(y0.item() - y) < epsilon:\n",
    "        break\n",
    "\n",
    "y0.backward()\n",
    "print(\"x0: \", x0, \"y0: \", y0, \"x0.grad: \", x0.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7ad445-cf5b-49c1-9b40-a097494242e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6223c72-1ac4-4ddf-93e3-0988b012bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#梯度最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1962f63f-e005-471c-bc28-93e947cb9dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: x = 0.24, f(x) = 4.499925759999999\n",
      "Iteration 100: x = 0.9999249570513495, f(x) = 0.10000002252746701\n",
      "Iteration 200: x = 0.9999999820510311, f(x) = 0.1000000000000013\n",
      "Iteration 300: x = 0.9999999999957063, f(x) = 0.1\n",
      "Iteration 400: x = 0.999999999999999, f(x) = 0.1\n",
      "Iteration 500: x = 0.9999999999999993, f(x) = 0.1\n",
      "Iteration 600: x = 0.9999999999999993, f(x) = 0.1\n",
      "Iteration 700: x = 0.9999999999999993, f(x) = 0.1\n",
      "Iteration 800: x = 0.9999999999999993, f(x) = 0.1\n",
      "Iteration 900: x = 0.9999999999999993, f(x) = 0.1\n",
      "Approximate minimum at x = 0.9999999999999993, f(x) = 0.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 定义函数\n",
    "def f(x):\n",
    "    return (x - 1)**2 * (x - 3)**2 + 0.1\n",
    "\n",
    "# 定义函数的导数\n",
    "def df(x):\n",
    "    return 4 * (x - 1) * (x - 3) * (x - 2)\n",
    "\n",
    "# 梯度下降算法\n",
    "def gradient_descent(starting_point, learning_rate, num_iterations):\n",
    "    x = starting_point\n",
    "    for i in range(num_iterations):\n",
    "        grad = df(x)\n",
    "        x = x - learning_rate * grad\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}: x = {x}, f(x) = {f(x)}\")\n",
    "    return x\n",
    "\n",
    "# 初始点，学习率和迭代次数\n",
    "starting_point = 0\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "# 运行梯度下降算法\n",
    "min_x = gradient_descent(starting_point, learning_rate, num_iterations)\n",
    "print(f\"Approximate minimum at x = {min_x}, f(x) = {f(min_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70ea348-15b8-4bf0-9d6c-d22172d186f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
